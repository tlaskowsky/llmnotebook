{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fb96424-d83f-44d2-8e3a-00428270aa05",
   "metadata": {},
   "source": [
    "#\n",
    "# ---------------------------------------------------------------------------------\n",
    "#  FFFFF   A   QQQQ      BBBB   OOO  TTTTT\n",
    "#  F      A A  Q  Q      B  B  O   O   T\n",
    "#  FFF   AAAAA Q  Q      BBBB  O   O   T\n",
    "#  F     A   A Q  Q      B  B  O   O   T\n",
    "#  F     A   A  QQ Q     BBBB   OOO    T\n",
    "# \n",
    "#  Welcome to the \"Build Your First FAQ Language Model\" Lab!\n",
    "#\n",
    "#  In this notebook, you will build a complete language model from scratch\n",
    "#  and train it to be a simple FAQ chatbot.\n",
    "#\n",
    "# ---------------------------------------------------------------------------------\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e96216-0332-4f8b-ab25-f7ee87e68e98",
   "metadata": {},
   "source": [
    "# =================================================================================\n",
    "#  ✅ Part 1: Setup and Data Preparation\n",
    "# =================================================================================\n",
    "#\n",
    "#  First, we need to prepare our data. The \"fuel\" for our AI model is a text file\n",
    "#  containing questions and answers. We will also import the necessary libraries.\n",
    "#\n",
    "# ---------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c61de0c-6128-4081-af96-a295ddf813f2",
   "metadata": {},
   "source": [
    "# --- Step 1.1: Import Libraries ---\n",
    "# We'll be using PyTorch for building our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b8fad1a7-e653-45ca-b89a-7ed45ad975f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8918fc7e-a687-4ffb-90b0-219219e350ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1.2: Create and Load the Dataset ---\n",
    "# In a real workshop, you would have students generate this text file using the\n",
    "# prompt template and save it as 'faq.txt'. For this example, we'll define it here.\n",
    "#\n",
    "# ----------------- YOUR FAQ DATA GOES HERE -----------------\n",
    "#\n",
    "#  Instructions:\n",
    "#  1. Use the prompt template provided in the workshop to generate your FAQ content.\n",
    "#  2. Paste the generated text into the triple-quoted string below.\n",
    "#  3. Make sure the file is saved in the same directory as this notebook.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08766fe0-4358-4c9b-80c5-a0353c1cfa7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "faq_text = \"\"\"\n",
    "Q: What are the store hours?\n",
    "A: Our store is open from 9 AM to 8 PM, Monday to Saturday.\n",
    "\n",
    "Q: What is the return policy?\n",
    "A: You can return any item within 30 days of purchase with a valid receipt.\n",
    "\n",
    "Q: Do you offer gift wrapping?\n",
    "A: Yes, we offer complimentary gift wrapping for all in-store purchases.\n",
    "\n",
    "Q: Where are you located?\n",
    "A: We are located at 123 Main Street, Anytown, USA.\n",
    "\n",
    "Q: Can I place an order online?\n",
    "A: Yes, you can place an order through our website at www.example-store.com.\n",
    "\n",
    "Q: What payment methods do you accept?\n",
    "A: We accept all major credit cards, debit cards, and mobile payments.\n",
    "\n",
    "Q: Is there parking available?\n",
    "A: Yes, there is a free parking lot available for all our customers behind the store.\n",
    "\n",
    "Q: Do you have a loyalty program?\n",
    "A: Yes, you can sign up for our free loyalty program to earn points on every purchase.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9f9b3c-7b8e-47fc-b876-f9f6863e400f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1.2: Read data from file ---\n",
    "# Read `faq.txt`file created by the student\n",
    "#\n",
    "# Uncomment below\n",
    "#with open('faq.txt', 'r', encoding='utf-8') as f:\n",
    "#    faq_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b242d374-58c7-429c-b1e4-e6a9dc43d727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------\n",
    "\n",
    "# Let's see the data we're working with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7112c4cb-1f23-44ba-98a7-0a82d392d343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Sample of our dataset ---\n",
      "\n",
      "Q: What are the store hours?\n",
      "A: Our store is open from 9 AM to 8 PM, Monday to Saturday.\n",
      "\n",
      "Q: What is the return policy?\n",
      "A: You can return any item within 30 days of purchase with a valid receipt.\n",
      "\n",
      "Q:\n",
      "-----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Sample of our dataset ---\")\n",
    "print(faq_text[:200])\n",
    "print(\"-----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b53e756-598f-49cd-b453-0bc66412504c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1.3: Create the Vocabulary ---\n",
    "# Our model can't understand letters. It only understands numbers. So, we need to\n",
    "# create a \"vocabulary\" and map each unique character to a unique integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "521caca4-d7f0-4302-8100-8725a31703ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our vocabulary contains 49 unique characters.\n",
      "Vocabulary: \n",
      " ,-.012389:?ACDIMOPQSUWYabcdefghijklmnoprstuvwxy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(faq_text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(f\"Our vocabulary contains {vocab_size} unique characters.\")\n",
    "print(f\"Vocabulary: {''.join(chars)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c449b974-31be-4d6e-b187-45e90dde1b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the mapping from character to integer (stoi) and integer to character (itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3a31ebb-142b-425e-8ad0-293b8b57d118",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5952ada0-6e34-401d-a205-9f0519def441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our encoding and decoding functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0962154c-acde-45f1-939d-128daa75bf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34d9e12-7217-4754-9205-7aa5cc264d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test them out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fd9cd2c-0f67-463e-8d27-cffc20849bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 'store hours'\n",
      "Encoded: [42, 43, 39, 41, 29, 1, 32, 39, 44, 41, 42]\n",
      "Decoded: 'store hours'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_string = \"store hours\"\n",
    "encoded_string = encode(test_string)\n",
    "decoded_string = decode(encoded_string)\n",
    "print(f\"Original: '{test_string}'\")\n",
    "print(f\"Encoded: {encoded_string}\")\n",
    "print(f\"Decoded: '{decoded_string}'\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65faf90a-76d4-4b0c-b656-462d436296c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1.4: Tokenize the Dataset ---\n",
    "# Now, we'll convert our entire text dataset into a single sequence of numbers.\n",
    "# PyTorch uses a data structure called \"tensors\" to work with numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa91dce1-0651-46d3-80df-8672263f89d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: torch.Size([842])\n",
      "First 100 tokens: tensor([ 0, 20, 11,  1, 23, 32, 25, 43,  1, 25, 41, 29,  1, 43, 32, 29,  1, 42,\n",
      "        43, 39, 41, 29,  1, 32, 39, 44, 41, 42, 12,  0, 13, 11,  1, 18, 44, 41,\n",
      "         1, 42, 43, 39, 41, 29,  1, 33, 42,  1, 39, 40, 29, 38,  1, 30, 41, 39,\n",
      "        37,  1, 10,  1, 13, 17,  1, 43, 39,  1,  9,  1, 19, 17,  2,  1, 17, 39,\n",
      "        38, 28, 25, 48,  1, 43, 39,  1, 21, 25, 43, 44, 41, 28, 25, 48,  4,  0,\n",
      "         0, 20, 11,  1, 23, 32, 25, 43,  1, 33])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(faq_text), dtype=torch.long)\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "print(f\"First 100 tokens: {data[:100]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208a196f-7230-4460-9ed2-68f7bbac6bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1.5: Split Data into Training and Validation Sets ---\n",
    "# We use most of our data for training the model (learning) and a small part\n",
    "# for validation (to check if it's learning correctly without just memorizing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "790959e9-1a80-48c2-bf92-e384bcdd6004",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9 * len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2035782-2c87-42b8-ba56-fc6b3f7ffd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "#  ✅ Part 2: Understanding Context and Batches\n",
    "# =================================================================================\n",
    "#\n",
    "#  A language model learns by seeing a chunk of text (context) and trying to\n",
    "#  predict the very next character.\n",
    "#\n",
    "# ---------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727854a1-d53f-4b37-9e96-b877ed7d2867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2.1: Define Context Size ---\n",
    "# `block_size` is the maximum length of the context the model can see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e85218f-d2fa-4334-a06c-495b2cdf5d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A single training example (x): [0, 20, 11, 1, 23, 32, 25, 43, 1, 25, 41, 29, 1, 43, 32, 29, 1, 42, 43, 39, 41, 29, 1, 32, 39, 44, 41, 42, 12, 0, 13, 11, 1, 18, 44, 41, 1, 42, 43, 39, 41, 29, 1, 33, 42, 1, 39, 40, 29, 38, 1, 30, 41, 39, 37, 1, 10, 1, 13, 17, 1, 43, 39, 1]\n",
      "The target for each character in x (y): [20, 11, 1, 23, 32, 25, 43, 1, 25, 41, 29, 1, 43, 32, 29, 1, 42, 43, 39, 41, 29, 1, 32, 39, 44, 41, 42, 12, 0, 13, 11, 1, 18, 44, 41, 1, 42, 43, 39, 41, 29, 1, 33, 42, 1, 39, 40, 29, 38, 1, 30, 41, 39, 37, 1, 10, 1, 13, 17, 1, 43, 39, 1, 9]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "block_size = 64\n",
    "print(f\"A single training example (x): {train_data[:block_size].tolist()}\")\n",
    "print(f\"The target for each character in x (y): {train_data[1:block_size+1].tolist()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57880ae4-182a-4801-a7f6-1c296ca246cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2.2: Create a Batch Function ---\n",
    "# We train the model on small, random chunks of data called \"batches\". This\n",
    "# helps the training process be more efficient and stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3160d2f9-3f00-44d2-a8a5-59763d54658b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 # How many independent sequences will we process in parallel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39b69ca9-ee92-4226-9cf8-e464db780ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    # Generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4898b7-38bb-4377-8098-7b36d97f619c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see a sample batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "29bec0aa-06d6-4604-8437-386c11b09468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Sample Batch ---\n",
      "Inputs (xb) shape: torch.Size([32, 64])\n",
      "Targets (yb) shape: torch.Size([32, 64])\n",
      "--------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xb, yb = get_batch('train')\n",
    "print(\"--- Sample Batch ---\")\n",
    "print(f\"Inputs (xb) shape: {xb.shape}\")\n",
    "print(f\"Targets (yb) shape: {yb.shape}\")\n",
    "print(\"--------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdd3fbe-4f0d-41e8-9faa-4a31a4876ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "#  ✅ Part 3: Building the Transformer Model (From Scratch!)\n",
    "# =================================================================================\n",
    "#\n",
    "#  This is the most exciting part! We will build the core components of the\n",
    "#  Transformer architecture, which is the foundation of models like GPT.\n",
    "#\n",
    "# ---------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48087c20-4125-4ec9-aa74-0ff74b7a06dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Hyperparameters ---\n",
    "# These are the settings for our model. You can experiment with these later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f0b38ef-640b-44f7-9521-b49e814c0270",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 128       # The size of the embedding for each character\n",
    "n_head = 4         # The number of attention heads\n",
    "n_layer = 4        # The number of transformer blocks\n",
    "dropout = 0.2      # A regularization technique to prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45aba1ea-df37-4671-9d84-aed267506d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# --- Step 3.1: The Self-Attention Head ---\n",
    "# This is the fundamental component. An \"attention head\" allows the model to look\n",
    "# at other characters in the context and decide which ones are most important\n",
    "# for predicting the next character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51d50d70-ff50-449b-adef-f4013091c61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9220ab55-3c68-4284-b516-efbcceca5b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3.2: Multi-Head Attention ---\n",
    "# To make the model more powerful, we use multiple attention heads in parallel\n",
    "# and combine their results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c8dbfe6f-bff1-47e7-b6d5-0e5d53863142",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78010c76-c042-4ee5-92b5-a803a79afbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3.3: The Feed-Forward Network ---\n",
    "# After the attention mechanism, each character's representation is passed\n",
    "# through a simple neural network to process the information gathered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fbf0b282-97ea-4c7b-a8e9-9c862f7d89df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71253e5e-6400-4851-88d1-ffae2ba180ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3.4: The Transformer Block ---\n",
    "# Now we combine the attention and feed-forward components into a single\n",
    "# \"Transformer Block\". A real LLM is just many of these blocks stacked together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "028678e5-e89d-4482-84be-84c12b746592",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad80eb3-d531-4adf-b447-b4a6b0172fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3.5: The Full Language Model ---\n",
    "# Finally, we assemble everything into our complete language model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c4ed3444-3e97-45f0-bebd-cdc64fc50500",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e241324c-43e9-452c-a622-6bad92435b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create an instance of our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "10bd15de-d6d1-4748-b2bb-146d8196787c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Model created successfully!\n"
     ]
    }
   ],
   "source": [
    "model = LanguageModel()\n",
    "print(\"Language Model created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8591f91b-4e29-415e-b6ad-cfe796c4eb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "#  ✅ Part 4: Training the Model\n",
    "# =================================================================================\n",
    "#\n",
    "#  Now we'll feed our data to the model and let it learn. This process involves\n",
    "#  showing the model batches of data, calculating how \"wrong\" its predictions\n",
    "#  are (the \"loss\"), and then slightly adjusting its internal parameters to\n",
    "#  make it better.\n",
    "#\n",
    "# ---------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20bed23-be9b-450c-bc8a-3ff720af8459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4.1: Create the Optimizer ---\n",
    "# The optimizer is the algorithm that adjusts the model's parameters.\n",
    "# AdamW is a popular and effective choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "600ab038-4732-4c0b-a82e-5c571e0ad289",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeea64a4-eb27-43b3-8159-26af49165235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4.2: The Training Loop ---\n",
    "# This loop will run for a set number of steps. In each step, we'll get a\n",
    "# batch of data, ask the model for a prediction, and update the model.\n",
    "#\n",
    "# NOTE: This will take a few minutes to run!\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "65201abc-b992-45d5-a6dd-b837357fc421",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters = 5000 # How many training steps? (More is better, but takes longer)\n",
    "eval_interval = 500 # How often to check the validation loss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "92ddf46c-962c-4a76-902e-5d18e7afd7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Training ---\n",
      "step 0: train loss 4.0805, val loss 4.0609\n",
      "step 500: train loss 0.0755, val loss 5.1587\n",
      "step 1000: train loss 0.0627, val loss 4.7291\n",
      "step 1500: train loss 0.0612, val loss 5.0100\n",
      "step 2000: train loss 0.0591, val loss 5.1008\n",
      "step 2500: train loss 0.0593, val loss 4.9941\n",
      "step 3000: train loss 0.0584, val loss 4.9174\n",
      "step 3500: train loss 0.0588, val loss 5.2184\n",
      "step 4000: train loss 0.0572, val loss 5.0626\n",
      "step 4500: train loss 0.0562, val loss 5.0061\n",
      "--- Training Complete! ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Starting Training ---\")\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0:\n",
    "        # We'll create a function to estimate the loss to avoid repeating code\n",
    "        @torch.no_grad()\n",
    "        def estimate_loss():\n",
    "            out = {}\n",
    "            model.eval()\n",
    "            for split in ['train', 'val']:\n",
    "                losses = torch.zeros(200)\n",
    "                for k in range(200):\n",
    "                    X, Y = get_batch(split)\n",
    "                    logits, loss = model(X, Y)\n",
    "                    losses[k] = loss.item()\n",
    "                out[split] = losses.mean()\n",
    "            model.train()\n",
    "            return out\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"--- Training Complete! ---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcbc88d-def4-49a8-b93a-cbc898b787df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "#  ✅ Part 5: Generating Answers with our FAQ Bot!\n",
    "# =================================================================================\n",
    "#\n",
    "#  This is the moment of truth! Let's use our trained model to answer questions.\n",
    "#  We'll give it a question as a \"prompt\" and see what it generates.\n",
    "#\n",
    "# ---------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bfd22d-ff45-4525-b76a-d064d59d9043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 5.1: The Generation Function ---\n",
    "# Let's write a simple function to interact with our bot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "96f6fd5b-c268-4587-8d14-c7ceb36aa0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_bot(question):\n",
    "    \"\"\"\n",
    "    Takes a question string and uses the model to generate an answer.\n",
    "    \"\"\"\n",
    "    # Prepare the prompt for the model\n",
    "    prompt = f\"Q: {question}\\nA:\"\n",
    "    print(prompt, end='') # Print the prompt without a newline\n",
    "\n",
    "    # Encode the prompt and create a tensor\n",
    "    context = torch.tensor(encode(prompt), dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "    # Generate the answer\n",
    "    generated_output = model.generate(context, max_new_tokens=50)[0].tolist()\n",
    "\n",
    "    # Decode and print the result\n",
    "    answer = decode(generated_output)\n",
    "    # We only want the generated part, so we find where the answer starts\n",
    "    answer_part = answer[len(prompt):]\n",
    "    print(answer_part.split('Q:')[0]) # Stop printing if it starts a new question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1709b1b0-8a77-4256-9ee0-e7f1d831f7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 5.2: Let's test it! ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a46d7d57-e5f0-4488-83dc-5ec90761dd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Ask the FAQ Bot ---\n",
      "Q: what are the store hours?\n",
      "A: Our store is open from 9 AM to 8 PM, Monday to Sa\n",
      "Q: do you offer gift wrapping\n",
      "A: Yes, we offer complimentary gift wrapping for all\n",
      "Q: parking?\n",
      "A: Yes, there is a free parking lot available for al\n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Ask the FAQ Bot ---\")\n",
    "ask_bot(\"what are the store hours?\") # Using lowercase\n",
    "ask_bot(\"do you offer gift wrapping\") # Lowercase and no question mark\n",
    "ask_bot(\"return policy?\") # A more ambiguous, partial question\n",
    "print(\"-----------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5eb6a39-ea2c-4229-8755-46d2004807ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "#  ✅ Part 6: Generating Answers with our Interactive FAQ Bot!\n",
    "# =================================================================================\n",
    "#\n",
    "#  Now, let's create a simple user interface right here in the notebook\n",
    "#  so you can interact with your bot.\n",
    "#\n",
    "# ---------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb936967-af01-4bbe-87a3-47ca723a68fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 6.1: Build the User Interface ---\n",
    "# We will use the `ipywidgets` library to create a text box and a button.\n",
    "\n",
    "# Create the widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "94bd581e-e620-46e8-8e91-e1b6e400d2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_input = widgets.Text(\n",
    "    value='What are the store hours?',\n",
    "    placeholder='Type your question here',\n",
    "    description='Question:',\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(width='80%')\n",
    ")\n",
    "\n",
    "submit_button = widgets.Button(\n",
    "    description='Ask Bot',\n",
    "    button_style='success', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Click to get an answer',\n",
    "    icon='question-circle'\n",
    ")\n",
    "\n",
    "output_area = widgets.Output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da263d40-7ca1-40ac-97c2-bc7a30413ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 5.2: Define the Button Click Action ---\n",
    "# This function will run every time you click the \"Ask Bot\" button.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d54f0942-3305-45cf-a9f2-1e5d9959be74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_button_clicked(b):\n",
    "    with output_area:\n",
    "        output_area.clear_output() # Clear previous answers\n",
    "        question = question_input.value\n",
    "\n",
    "        # Prepare the prompt for the model\n",
    "        prompt = f\"Q: {question}\\nA:\"\n",
    "        print(prompt, end='')\n",
    "\n",
    "        # Encode the prompt and create a tensor\n",
    "        known_chars_prompt = ''.join([c for c in prompt if c in stoi])\n",
    "        context = torch.tensor(encode(known_chars_prompt), dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "        # Generate the answer\n",
    "        generated_output = model.generate(context, max_new_tokens=100)[0].tolist()\n",
    "\n",
    "        # Decode and print the result\n",
    "        answer = decode(generated_output)\n",
    "        answer_part = answer[len(known_chars_prompt):]\n",
    "        # Stop printing if it starts a new question or gets stuck in a loop\n",
    "        final_answer = answer_part.split('Q:')[0].split('\\n\\n')[0]\n",
    "        print(final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fbbfbad5-3ee9-451a-9dc3-8c5d7d568b8b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'on_button_clicked' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Link the button to the function\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m submit_button\u001b[38;5;241m.\u001b[39mon_click(\u001b[43mon_button_clicked\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'on_button_clicked' is not defined"
     ]
    }
   ],
   "source": [
    "# Link the button to the function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0a166d93-fa6f-41b7-88c7-7ee54b16d06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_button.on_click(on_button_clicked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb289ba9-230a-4a2b-a0d8-1443ecfbfe60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 5.3: Display the Bot! ---\n",
    "# Now, we display the UI elements. Type a question and click the button!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "34b0c2fc-0897-4281-9cdc-94152051c410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Interactive FAQ Bot ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "066046abbec4437d98e3a93f797fabb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='do you have parking?', description='Question:', layout=Layout(width='80%'), placeholder='Type your…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76fe1f70095644cbadc95c19689cadcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='Ask Bot', icon='question-circle', style=ButtonStyle(), tooltip='Cl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49789b16e8e84f20b26171726e2cce9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(outputs=({'name': 'stdout', 'text': 'Q: do you have parking?\\nA: Yes, there is a free parkilable for al…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"--- Interactive FAQ Bot ---\")\n",
    "display(question_input, submit_button, output_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b12a56-b9f7-46c1-a76e-15f28a38a129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Final Thoughts & Next Steps ---\n",
    "#\n",
    "# Congratulations! You've built a language model from scratch.\n",
    "#\n",
    "# This is a \"seed\" project. It's not perfect, but it demonstrates the core concepts.\n",
    "# To make it better, you could:\n",
    "#   1.  Add more data to your faq.txt file.\n",
    "#   2.  Train for more iterations (increase `max_iters`).\n",
    "#   3.  Experiment with the hyperparameters (e.g., `n_embd`, `n_head`, `n_layer`).\n",
    "#   4.  Build a simple user interface for it.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60fd596-8f6a-4b95-bf18-d90c890f38d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
